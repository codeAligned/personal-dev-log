- Features scope
    - Start with a whitelist of domains - (Should be dynamically updated)
    - Honor a blacklist - (Should be dynamically updated)
    - Honor robot.txt file on the domain you are calling


Goal: how would I design a modern web crawler?

How Robots.txt works
User-agent: msnbot
Crawl-delay: 120
Disallow: /*.xml$
Disallow: /buzz/*.xml$
Disallow: /category/*.xml$
Disallow: /mobile/
Disallow: *?s=lightbox
Disallow: /bfmp/

User-agent: *
Crawl-delay: 120
Disallow: /*.xml$
Disallow: /buzz/*.xml$
Disallow: /category/*.xml$
Disallow: /mobile/
Disallow: *?s=lightbox
Disallow: /bfmp/

User-agent: *
Disallow: /

User Cases
    - web crawler that travels through each site to collection information about products
    - [amazon.com] user create product information to site

Constraints and assumptions
    - Traffic is not evenly distributed
      - Some searches are very popular, while others are only executed once
    - The web crawler should not get stuck in an infinite loop
    - We get stuck in an infinite loop if the graph contains a cycle
    - 1 billion links to crawl
    - Pages need to be crawled regularly to ensure freshness
    - Average refresh rate of about once per week, more frequent for popular sites
    - 4 billion links crawled each month
    - Average stored size per web page: 500 KB
    - For simplicity, count changes the same as new pages
    - 1000 products loaded each second

back-of-envelop-calculation
  - 2,000,000 billion bytes stored in database in a month -> 2 PB of data / month --> 24 PB data --> 72 PB of data / 3 years
  - 1000 products loaded each second

# ==================================================
Part 2
    - see web_crawler_part2.png

# ==================================================
Part 3
  - prevent crwalers from getting into an infinite loop

def collect_data(self, urls, blacklisted):
    for url in urls:
      new_urls = []
      1. collect data on page
      self._collect(url)
      2. collect all urls of the page
      temp_arr = self._collect_urls(url)

      3. for each url, check and see if url is already in database and that is not expired . If so, replace it with new content, otherwise skip
      for new_url in temp_arr:
        if new_url in blacklisted_urls:
          continue

        if self._info_exists_and_not_expired(new_url):
          continue

        if self._url_disallowed(new_url):
          continue

        new_urls.append(new_url)

      5. if all criterias are satisfied, then access the link to collect information

        urls += new_urls


# ==================================================
Part 4
    - see web_crawler_part4.png